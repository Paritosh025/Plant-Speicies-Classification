# -*- coding: utf-8 -*-
"""DenseNet121.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jEMzKUkQ6MnfFYQbLJiM3ZrreU7cE6xV
"""

# =======================
# 1. Install dependencies
# =======================
!pip install -q kaggle pytorch-lightning efficientnet_pytorch opencv-python-headless

# ================================
# 2. Upload your Kaggle API key
# ================================
from google.colab import files
files.upload()   # <-- Upload kaggle.json from your Kaggle account

# ================================
# 3. Set up Kaggle & Download Data
# ================================
import os
os.makedirs('/root/.kaggle', exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# Download Plant Seedlings Classification dataset
!kaggle competitions download -c plant-seedlings-classification
!unzip -q plant-seedlings-classification.zip -d data/

# =======================
# 1. Check Data Structure
# =======================
import os

data_path = 'data/train'
classes = sorted(os.listdir(data_path))
print(f"Found {len(classes)} plant species (classes):")
print(classes)

# Show number of images per class
for cls in classes:
    n_imgs = len(os.listdir(os.path.join(data_path, cls)))
    print(f"{cls}: {n_imgs} images")

# =======================
# 2. Show Example Images
# =======================
import matplotlib.pyplot as plt
import cv2

fig, axs = plt.subplots(2, 6, figsize=(16, 6))
for idx, cls in enumerate(classes[:12]):
    img_path = os.path.join(data_path, cls, os.listdir(os.path.join(data_path, cls))[0])
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    axs[idx//6, idx%6].imshow(img)
    axs[idx//6, idx%6].set_title(cls)
    axs[idx//6, idx%6].axis('off')
plt.tight_layout()
plt.show()

import os
import shutil
import random
from tqdm import tqdm

# Set seed for reproducibility
random.seed(42)

original_data_dir = 'data/train'
split_base = 'data/split'
train_dir = os.path.join(split_base, 'train')
val_dir = os.path.join(split_base, 'val')

# Create split directories
for split_dir in [train_dir, val_dir]:
    if not os.path.exists(split_dir):
        os.makedirs(split_dir)

# Split ratio (e.g., 85% train, 15% val)
split_ratio = 0.85

# For each class, shuffle and split images
for cls in tqdm(os.listdir(original_data_dir), desc="Splitting data by class"):
    cls_folder = os.path.join(original_data_dir, cls)
    images = os.listdir(cls_folder)
    random.shuffle(images)
    n_train = int(len(images) * split_ratio)
    train_imgs = images[:n_train]
    val_imgs = images[n_train:]

    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)
    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)

    for img in train_imgs:
        shutil.copy(os.path.join(cls_folder, img), os.path.join(train_dir, cls, img))
    for img in val_imgs:
        shutil.copy(os.path.join(cls_folder, img), os.path.join(val_dir, cls, img))

print("Splitting complete!")

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import pytorch_lightning as pl

# Mean/std calculated from ImageNet; fine for transfer learning
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

class PlantSeedlingsDataModule(pl.LightningDataModule):
    def __init__(self, data_dir, batch_size=32, num_workers=2, img_size=224):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.img_size = img_size

    def setup(self, stage=None):
        # Training transforms: augment
        self.train_transforms = transforms.Compose([
            transforms.RandomResizedCrop(self.img_size),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
        ])
        # Validation: center crop only
        self.val_transforms = transforms.Compose([
            transforms.Resize((self.img_size, self.img_size)),
            transforms.ToTensor(),
            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
        ])

        self.train_dataset = datasets.ImageFolder(os.path.join(self.data_dir, "train"), self.train_transforms)
        self.val_dataset = datasets.ImageFolder(os.path.join(self.data_dir, "val"), self.val_transforms)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)

# Usage example:
data_module = PlantSeedlingsDataModule(data_dir='data/split', batch_size=32, num_workers=2)
data_module.setup()

batch = next(iter(data_module.train_dataloader()))
images, labels = batch
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
for i in range(8):
    img = images[i].permute(1,2,0).cpu().numpy() * IMAGENET_STD + IMAGENET_MEAN
    img = (img * 255).astype('uint8')
    plt.subplot(1, 8, i+1)
    plt.imshow(img)
    plt.axis('off')
plt.show()



import torch
import torch.nn as nn
import pytorch_lightning as pl
from torchvision import models
import os

class DenseNetClassifier(pl.LightningModule):
    def __init__(self, num_classes, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        # Load pretrained DenseNet-121
        self.model = models.densenet121(pretrained=True)
        in_features = self.model.classifier.in_features
        self.model.classifier = nn.Linear(in_features, num_classes)
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        images, targets = batch
        outputs = self(images)
        loss = self.criterion(outputs, targets)
        acc = (outputs.argmax(1) == targets).float().mean()
        self.log('train_loss', loss, on_epoch=True, prog_bar=True)
        self.log('train_acc', acc, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        images, targets = batch
        outputs = self(images)
        loss = self.criterion(outputs, targets)
        acc = (outputs.argmax(1) == targets).float().mean()
        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        self.log('val_acc', acc, on_epoch=True, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
        return optimizer

from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

# Count classes
num_classes = len(os.listdir('data/split/train'))

# Create data module (already defined)
data_module = PlantSeedlingsDataModule(data_dir='data/split', batch_size=64, num_workers=4)

# Define model
model = DenseNetClassifier(num_classes=num_classes, lr=1e-3)

# Callbacks
checkpoint_cb = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename='densenet121-best')
early_stop_cb = EarlyStopping(monitor='val_acc', patience=10, mode='max')

trainer = pl.Trainer(
    max_epochs=12,
    accelerator='gpu' if torch.cuda.is_available() else 'cpu',
    devices=1 if torch.cuda.is_available() else None,
    callbacks=[checkpoint_cb, early_stop_cb],
    precision=16,
    log_every_n_steps=50,
)

# Train!
trainer.fit(model, datamodule=data_module)

# Load the best model
best_model = DenseNetClassifier.load_from_checkpoint(checkpoint_cb.best_model_path, num_classes=num_classes)
best_model.eval()  # Set model to evaluation mode

# Evaluate on validation set
trainer.validate(best_model, datamodule=data_module)

batch = next(iter(data_module.val_dataloader()))
images, targets = batch
outputs = best_model(images)
preds = outputs.argmax(1)

import matplotlib.pyplot as plt
import numpy as np

IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])
IMAGENET_STD = np.array([0.229, 0.224, 0.225])

plt.figure(figsize=(14, 4))
class_names = sorted(os.listdir('data/split/train'))  # sorted ensures consistent ordering

for i in range(8):
    img = images[i].permute(1, 2, 0).cpu().numpy()
    img = img * IMAGENET_STD + IMAGENET_MEAN  # denormalize
    img = np.clip(img, 0, 1)  # clip values to valid range
    img = (img * 255).astype('uint8')
    plt.subplot(1, 8, i + 1)
    plt.imshow(img)
    plt.title(f"Pred: {class_names[preds[i]]}\nTrue: {class_names[targets[i]]}", fontsize=9)
    plt.axis('off')

plt.tight_layout()
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import pandas as pd

# Get predictions and calculate metrics
best_model.eval()
best_model = best_model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move model to correct device

all_preds = []
all_targets = []
total_loss = 0.0
total_samples = 0

criterion = nn.CrossEntropyLoss()

with torch.no_grad():
    for batch in data_module.val_dataloader():
        images, targets = batch
        device = next(best_model.parameters()).device
        images, targets = images.to(device), targets.to(device)

        outputs = best_model(images)
        loss = criterion(outputs, targets)

        preds = outputs.argmax(1)

        all_preds.extend(preds.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

        # Accumulate loss
        batch_size = images.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size

all_preds = np.array(all_preds)
all_targets = np.array(all_targets)

# Calculate metrics
accuracy = np.mean(all_preds == all_targets)
avg_loss = total_loss / total_samples

print(f"Validation Metrics:")
print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Loss: {avg_loss:.4f}")

# Get predictions
outputs = best_model(images)
preds = outputs.argmax(1)

# Move images back to CPU for visualization
images = images.cpu()

plt.figure(figsize=(15, 4))
IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406])
IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225])
for i in range(min(8, len(images))):  # Ensure we don't exceed batch size
    img = images[i].permute(1, 2, 0) * IMAGENET_STD + IMAGENET_MEAN
    img = torch.clamp(img, 0, 1)

    plt.subplot(1, 8, i+1)
    plt.imshow(img)
    color = 'green' if preds[i] == targets[i] else 'red'
    plt.title(f"Pred: {class_names[preds[i]]}\nTrue: {class_names[targets[i]]}",
             fontsize=9, color=color)
    plt.axis('off')

plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)')
plt.tight_layout()
plt.show()

# At the end, save results for DenseNet
torch.save({
    'model_name': 'DenseNet121',
    'accuracy': accuracy,
    'loss': avg_loss,
    'best_model_path': checkpoint_cb.best_model_path
},  'densenet121_results.pt')

